{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb65193-a360-4dca-9866-1e5d94a9790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e51ea43-a410-4c91-b12b-c917ae2d8a16",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 41 (2673206303.py, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 44\u001b[1;36m\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'if' statement on line 41\n"
     ]
    }
   ],
   "source": [
    "## lets try and make neural network class\n",
    "# input_data, output_data, [number of neurons and size], activation_function, epochs, learning_rate,  \n",
    "# optomizers: ADAMW\n",
    "class Neural_Network:\n",
    "    def __init__(self, input_data, output_data, hidden_layers, activation_functions, epochs, learning_rate, optimizer, loss_function, batch_size):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        self.hidden_layers = hidden_layers # an array of length number of layers. each value is the number \n",
    "        self.activation_function = activation_function # a list of Tuples (a, x) where a is the activation type ('relu', 'sigmoid', 'tanh', 'lrelu') and x is the alpha range(0, 1) for lrelu \n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.initialize_weights()\n",
    "        # how should the data be inputed...\n",
    "        # here you can split the data into training and testing data\n",
    "        # I am going to make my dataset below and that will make it easier to find out what the shape is\n",
    "        \n",
    "        # STILL NEED TO CREATE MINIBATCHES\n",
    "        ## building the model\n",
    "    def initialize_weights(self): \n",
    "        last_layer = self.input_data.shape[1]\n",
    "        for i, size in enumerate(self.hidden_layers):\n",
    "            self.weights.append(0.1 * np.random.randn(last_layer, size))\n",
    "            self.biases.append(np.zeros((1, size)))\n",
    "            last_layer = self.hidden_layers[i]\n",
    "            \n",
    "        self.weights.append(0.1 * np.random.randn(last_layer, self.output_data.shape[1])) # wacking in output data here means we have to clean before\n",
    "        self.biases.append(np.zeros((1, self.output_data.shape[1])))\n",
    "        \n",
    "                \n",
    "        # build the 1st layer with inputs\n",
    "    def activation(self, input, type):#Relu, sigmoid, tanh\n",
    "        \n",
    "        if type[0] == \"relu\":\n",
    "            return np.maximum(0, input)\n",
    "        elif type[0] == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-inputs))\n",
    "        elif type[0] == \"tanh\":\n",
    "            return np.tanh(input)\n",
    "        elif type[0] == \"lrelu\": # THink about how you can have an alpha parameter when you want it \n",
    "            return np.maximum(type[1] * input, input) \n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "\n",
    "    def activation_derivative(self, input, type): # check this you should remember what function you had\n",
    "        \n",
    "        if type[0] == 'relu':\n",
    "            return np.where(input > 0, 1, 0)\n",
    "        elif type[0] == 'sigmoid':\n",
    "            sig = self.activation(input, 'sigmoid')\n",
    "            return sig * (1 - sig)\n",
    "        elif type[0] == 'tanh':\n",
    "            return 1 - np.tanh(input)**2\n",
    "        elif type[0] == 'lrelu':\n",
    "            dx = np.ones_like(x)\n",
    "            dx[x <= 0] = type[1]   # THink about how you can have an alpha parameter when you want it \n",
    "            return dx\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function derivative\")\n",
    "                              \n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        activations = []\n",
    "        inputs = inputs\n",
    "        for i, size in enumerate(self.hidden_layers):\n",
    "            z = np.dot(inputs, self.weights[i] + self.biases[i])\n",
    "            a = self.activation(z, self.activation_function[i])\n",
    "            activations.append((z, a))\n",
    "            inputs = a\n",
    "            \n",
    "        z = np.dot(input, self.weights[-1]) + self.biases[-1]\n",
    "        activations.append((z,))\n",
    "        return activations ### weights and biases are recorded in self.weights and self.biases, outputs recorded in activations\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        \n",
    "    def loss(self, predictions, targets):\n",
    "        if self.loss_function == 'mse':\n",
    "            sample_losses = np.mean((predictions - targets) ** 2, axis=-1)\n",
    "            data_loss = np.mean(sample_losses)\n",
    "        else:\n",
    "            raise Exception(\"Invalid loss function\")\n",
    "\n",
    "\n",
    "    def update_weights(self, grads):\n",
    "\n",
    "        if self.optimizer == \"SGD\":\n",
    "            for grad, weight, bias in grads:\n",
    "            weight -= self.learning_rate * np.dot(grad.T, weight)\n",
    "            bias -= self.learning_rate * np.sum(grad, axis=0, keepdims=True)\n",
    "        elif self.optimizer == \"AdamW\":\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\"Invalid optimizer entered\")\n",
    "\n",
    "\n",
    "    \n",
    "    def train():\n",
    "        pass\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        activations = self.forward(inputs)\n",
    "        return activations[-1][0]\n",
    "## methods: build, train, use (give predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9430eae-8a7b-4da7-950b-f0648f186c96",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tadaaaa",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTadaaaa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Tadaaaa"
     ]
    }
   ],
   "source": [
    "gillespie = Neural_Network()\n",
    "\n",
    "\n",
    "gillespie.train()\n",
    "gillespie.use()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12cf39-83bd-4db3-ac42-9ff798b3b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):3\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0328cefb-22fa-4d49-8dcf-e58085c38630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initial states\n",
    "initial_states = [0, 3, 0, 0, 0, 0]\n",
    "\n",
    "# Parameters (example values, adjust as necessary)\n",
    "transcription_rate = 50\n",
    "basal_transcript_rate = 0.01\n",
    "mRNA_degradation_rate = 1\n",
    "translation_rate = 5\n",
    "protein_degradation_rate = 0.1\n",
    "n = 2  # Hill coefficient\n",
    "\n",
    "# Gillespie function\n",
    "def gillespie(initial_states, transcription_rate, basal_transcript_rate, mRNA_degradation_rate, translation_rate, protein_degradation_rate, n, max_time):\n",
    "    t = 0\n",
    "    state = np.array(initial_states)\n",
    "    times = [t]\n",
    "    states = [state.copy()]\n",
    "\n",
    "    while t < max_time:\n",
    "        TetR_mRNA, TetR_protein, cI_mRNA, cI_protein, LacI_mRNA, LacI_protein = state\n",
    "\n",
    "        # Propensities\n",
    "        a = np.array([\n",
    "            (transcription_rate / (1 + LacI_protein ** n)) + basal_transcript_rate,  # production of TetR_mRNA\n",
    "            mRNA_degradation_rate * TetR_mRNA,  # degradation of TetR_mRNA\n",
    "            translation_rate * TetR_mRNA,  # production of TetR protein\n",
    "            protein_degradation_rate * TetR_protein,  # degradation of TetR protein\n",
    "\n",
    "            (transcription_rate / (1 + TetR_protein ** n)) + basal_transcript_rate,  # production of cI_mRNA\n",
    "            mRNA_degradation_rate * cI_mRNA,  # degradation of cI_mRNA\n",
    "            translation_rate * cI_mRNA,  # production of cI protein\n",
    "            protein_degradation_rate * cI_protein,  # degradation of cI protein\n",
    "\n",
    "            (transcription_rate / (1 + cI_protein ** n)) + basal_transcript_rate,  # production of LacI_mRNA\n",
    "            mRNA_degradation_rate * LacI_mRNA,  # degradation of LacI_mRNA\n",
    "            translation_rate * LacI_mRNA,  # production of LacI protein\n",
    "            protein_degradation_rate * LacI_protein  # degradation of LacI protein\n",
    "        ])\n",
    "\n",
    "        a0 = np.sum(a)\n",
    "        if a0 == 0:\n",
    "            break\n",
    "\n",
    "        r1 = np.random.random()\n",
    "        tau = (1 / a0) * np.log(1 / r1)\n",
    "\n",
    "        r2 = np.random.uniform(0, 1)\n",
    "        cumulative_sum = np.cumsum(a)\n",
    "        reaction_index = np.searchsorted(cumulative_sum, r2 * a0)\n",
    "\n",
    "        # Update the system\n",
    "        state_changes = [\n",
    "            [1, 0, 0, 0, 0, 0],   # mRNA_TetR production\n",
    "            [-1, 0, 0, 0, 0, 0],  # mRNA_TetR degradation\n",
    "            [0, 1, 0, 0, 0, 0],   # protein_TetR production\n",
    "            [0, -1, 0, 0, 0, 0],  # protein_TetR degradation\n",
    "\n",
    "            [0, 0, 1, 0, 0, 0],   # mRNA_cI production\n",
    "            [0, 0, -1, 0, 0, 0],  # mRNA_cI degradation\n",
    "            [0, 0, 0, 1, 0, 0],   # protein_cI production\n",
    "            [0, 0, 0, -1, 0, 0],  # protein_cI degradation\n",
    "\n",
    "            [0, 0, 0, 0, 1, 0],   # mRNA_LacI production\n",
    "            [0, 0, 0, 0, -1, 0],  # mRNA_LacI degradation\n",
    "            [0, 0, 0, 0, 0, 1],   # protein_LacI production\n",
    "            [0, 0, 0, 0, 0, -1],  # protein_LacI degradation\n",
    "        ]\n",
    "\n",
    "        state += state_changes[reaction_index]\n",
    "        t += tau\n",
    "\n",
    "        times.append(t)\n",
    "        states.append(state.copy())\n",
    "\n",
    "    return np.array(times), np.array(states)\n",
    "\n",
    "\n",
    "# Simulation parameters\n",
    "max_time = 1000\n",
    "\n",
    "# Run the simulation\n",
    "times, states = gillespie(initial_states, transcription_rate, basal_transcript_rate, mRNA_degradation_rate, translation_rate, protein_degradation_rate, n, max_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff732e77-c482-40dd-9c4f-a6acdd894663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(*args):\n",
    "    print(args[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e15620-5973-4a56-b052-90aa7d866e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "nimby\n"
     ]
    }
   ],
   "source": [
    "test(5,6,7,7)\n",
    "test('nimby')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff6f98-66ae-4953-904a-e26c3e8691c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
