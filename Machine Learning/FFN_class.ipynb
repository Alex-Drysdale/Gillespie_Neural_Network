{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b591e6e2-861a-40b0-a612-398c7d06af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71cc35aa-a0d0-40cd-94bc-985364bca6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data sets\n",
    "\n",
    "X_train = np.random.uniform(-1, 1, (5, 20)) #25, 20, 5\n",
    "Y_train = np.random.uniform(0, 1, (2, 20)) # 25, 20, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c2ae7711-e3a2-446d-b1a8-9902e30f72dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the class FFN:\n",
    "# a time stepper that takes in (input_data, output_data, hidden_layers (list), activation_function (list of tuples), epochs, learning rate)\n",
    "# going to assume mean squared error and stochastic gradient descent.\n",
    "class FFN:\n",
    "    def __init__(self, input_data, output_data, hidden_layers, activation_functions, epochs, learning_rate, batch_size):\n",
    "        self.input_data = input_data ## make sure this is a numpy array\n",
    "        self.output_data = output_data\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activation_functions = activation_functions\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.weights_dictionary = {}\n",
    "        self.activations_dictionary = {}\n",
    "        self.initialize()\n",
    "\n",
    "\n",
    "    def initialize(self):\n",
    "        last_layer = self.input_data.shape[0]\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            #print(i)\n",
    "            self.weights_dictionary[f'w{i}'] = 0.1 * np.random.randn(self.hidden_layers[i], last_layer)\n",
    "            self.weights_dictionary[f'b{i}'] = np.zeros((self.hidden_layers[i], 1))\n",
    "            last_layer = self.hidden_layers[i]\n",
    "        self.weights_dictionary['last_weight'] = 0.1 * np.random.randn(self.output_data.shape[0], last_layer)\n",
    "        self.weights_dictionary['last_bias'] = np.zeros((self.output_data.shape[0], 1))\n",
    "        \n",
    "                \n",
    "    def forward(self, input_batch): #setting up to be called within the training method for each batch\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            z = np.dot(self.weights_dictionary[f'w{i}'], input_batch) + self.weights_dictionary[f'b{i}'] # so weights of first layer are \n",
    "            #print(f' input {input_batch.shape}', f\"biases {self.weights_dictionary[f'b{i}'].shape}\", f\"weights {self.weights_dictionary[f'w{i}'].shape}\")\n",
    "            a = self.activation(z, self.activation_functions[i]) # should have shape (neurons in layer 1, batch size)\n",
    "            self.activations_dictionary[f'z{i + 1}'] = z #activation (prefunction) of the first hidden layer is added as z1 \n",
    "            self.activations_dictionary[f'a{i + 1}'] = a #activation of first layer added as a1\n",
    "            input_batch = a\n",
    "        #print(f'weights {self.weights_dictionary[\"last_weight\"].shape}', f'input {input_batch.shape}', f'biases {self.weights_dictionary[\"last_bias\"].shape}')   \n",
    "        a = np.dot(self.weights_dictionary['last_weight'], input_batch) + self.weights_dictionary[\"last_bias\"]\n",
    "        self.activations_dictionary['activation_output'] = a\n",
    "        \n",
    "        ## so much cleaner instead of having to index a load of lists and potentially reverse them etc.\n",
    "        \n",
    "    def activation(self, a, activation_function):\n",
    "        if activation_function[0] == \"relu\":\n",
    "            return np.maximum(0, a)\n",
    "        elif activation_function[0] == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-a))\n",
    "        elif activation_function[0] == \"tanh\":\n",
    "            return np.tanh(a)\n",
    "        elif activation_function[0] == \"lrelu\": # THink about how you can have an alpha parameter when you want it \n",
    "            return np.maximum(activation_function[1] * a, a) \n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "\n",
    "    def activation_derivative(self, a, activation_function): # check this you should remember what function you had\n",
    "        \n",
    "        if activation_function[0] == 'relu':\n",
    "            return np.where(a > 0, 1, 0)\n",
    "        elif activation_function[0] == 'sigmoid':\n",
    "            sig = self.activation(a, ('sigmoid', 0))\n",
    "            return sig * (1 - sig)\n",
    "        elif activation_function[0] == 'tanh':\n",
    "            return 1 - np.tanh(a)**2\n",
    "        elif activation_function[0] == 'lrelu':\n",
    "            dx = np.ones_like(input)\n",
    "            dx[a <= 0] = activation_function[1]   # THink about how you can have an alpha parameter when you want it \n",
    "            return dx\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function derivative\")\n",
    "\n",
    "    def backward(self, dvalues): ## just need the dictionary key for the output activations and the output batch\n",
    "        ## computing the loss gradient here\n",
    "        i = len(self.hidden_layers)\n",
    "        #print( f'loss gradients shape {dvalues.shape}', f\"output {self.activations_dictionary[f'a{i}'].T.shape}\")\n",
    "        self.weights_dictionary[\"dweights last_weight\"] = np.dot(dvalues, self.activations_dictionary[f'a{i}'].T) #if this dont work switch them...\n",
    "        self.weights_dictionary[\"dbiases last_bias\"] = np.sum(dvalues, axis = 1, keepdims=True)\n",
    "        dinputs = np.dot(self.weights_dictionary[\"dweights last_weight\"].T, dvalues)\n",
    "        #print(f\"dweights shape{self.weights_dictionary[f'dweights last_weight'].shape}\", f\"dbias shape{self.weights_dictionary[f'dbiases last_bias'].shape}\", f\"dinputs shape{dinputs.shape}\")\n",
    "        # for i in range(len(self.hidden_layers), 0, -1):\n",
    "        #     print(i)\n",
    "        #     self.weights_dictionary[f'dweights{i-1}'] = np.dot(dinputs, self.activations_dictionary[f'a{i-1}'].T)\n",
    "        #     self.weights_dictionary[f'dbiases{i-1}'] = np.sum(dinputs, axis = 0, keepdims=True)\n",
    "        #     dinputs = np.dot(self.weights_dictionary[f'dweights{i-1}'].T, dinputs)\n",
    "        #     dinputs *= \n",
    "        #     print(f\"dweights shape{self.weights_dictionary[f'dweights{i-1}'].shape}\", f\"dbias shape{self.weights_dictionary[f'dbiases{i-1}'].shape}\", f\"dinputs shape{dinputs.shape}\")\n",
    "\n",
    "        for i in range(len(self.hidden_layers)-1, -1, -1):\n",
    "            #print(i)\n",
    "            self.weights_dictionary[f'dweights{i}'] = np.dot(dinputs, self.activations_dictionary[f'a{i}'].T if i > 0 else self.input_data.T)\n",
    "            self.weights_dictionary[f'dbiases{i}'] = np.sum(dinputs, axis=1, keepdims=True)\n",
    "            if i > 0:\n",
    "                dinputs = np.dot(self.weights_dictionary[f'w{i}'].T, dinputs)\n",
    "                dinputs *= self.activation_derivative(self.activations_dictionary[f'z{i}'], self.activation_functions[i-1])\n",
    "            #print(f\"dweights shape{self.weights_dictionary[f'dweights{i}'].shape}\", f\"dbias shape{self.weights_dictionary[f'dbiases{i}'].shape}\", f\"dinputs shape{dinputs.shape}\")\n",
    "\n",
    "        \n",
    "        \n",
    "    def compute_loss(self, predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2) # the loss function averaged out for normalisation\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def loss_backwards(self, predictions, targets):\n",
    "        return 2 * (predictions - targets) / len(targets) # the loss function averaged out for normalisation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        epoch_data = []\n",
    "        loss_history = []\n",
    "        for epoch in range(self.epochs):\n",
    "            for start in range(0, self.input_data[1], self.batch_size):\n",
    "                start = start\n",
    "                end = start + self.batch_size\n",
    "                #print(f'end {end}')\n",
    "                input_batch = self.input_data[start:end]\n",
    "                #print(f'batchinputs {batch_inputs.shape}')\n",
    "                output_batch = self.output_data[start:end]\n",
    "                #print(f'batchoutputs {len(batch_outputs)}')\n",
    "            \n",
    "                input_batch = self.input_data # should be minibatches but for now no\n",
    "                output_batch = self.output_data # same here\n",
    "                self.forward(input_batch)\n",
    "                loss = self.compute_loss(self.activations_dictionary[\"activation_output\"], output_batch) # should be a function calling usual MSE\n",
    "                dvalues = self.loss_backwards(self.activations_dictionary[\"activation_output\"], output_batch)\n",
    "                self.backward(dvalues)\n",
    "                self.update()\n",
    "    \n",
    "                if epoch % 2 == 0:\n",
    "                    print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "                    epoch_data.append(epoch)\n",
    "                    loss_history.append(loss)\n",
    "\n",
    "        plt.plot(epoch_data, loss_history)\n",
    "        plt.xlabel(\"Iteration Number\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Iteration Number vs Loss\")\n",
    "        plt.show()\n",
    "        \n",
    "    def update(self):\n",
    "        for i in range(0, len(self.hidden_layers)):\n",
    "            self.weights_dictionary[f'w{i}'] -= self.learning_rate * self.weights_dictionary[f'dweights{i}']\n",
    "            self.weights_dictionary[f'b{i}'] -= self.learning_rate * self.weights_dictionary[f'dbiases{i}']\n",
    "        self.weights_dictionary[\"last_weight\"] -= self.learning_rate * self.weights_dictionary[\"dweights last_weight\"]\n",
    "        self.weights_dictionary[\"last_bias\"] -= self.learning_rate * self.weights_dictionary[\"dbiases last_bias\"]\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "38acb499-dea7-43e2-9d7e-106bb67ecbaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m tester\u001b[38;5;241m.\u001b[39mforward(X_train)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#tester.backward(tester.activation_derivative(Y_train, tester.activation_functions[0]))\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tester\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[160], line 117\u001b[0m, in \u001b[0;36mFFN.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_data[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[0;32m    118\u001b[0m         start \u001b[38;5;241m=\u001b[39m start\n\u001b[0;32m    119\u001b[0m         end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "tester = FFN(X_train, Y_train, [30, 30], [(\"relu\", 0), (\"relu\", 0)], 100, 0.1, 10)\n",
    "tester.forward(X_train)\n",
    "#tester.backward(tester.activation_derivative(Y_train, tester.activation_functions[0]))\n",
    "tester.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
