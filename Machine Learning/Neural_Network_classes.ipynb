{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb65193-a360-4dca-9866-1e5d94a9790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "3e51ea43-a410-4c91-b12b-c917ae2d8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets try and make neural network class\n",
    "# input_data, output_data, [number of neurons and size], activation_function, epochs, learning_rate,  \n",
    "# optomizers: ADAMW\n",
    "class Neural_Network:\n",
    "    def __init__(self, input_data, output_data, hidden_layers, activation_functions, epochs, learning_rate, optimizer, loss_function, batch_size):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        self.hidden_layers = hidden_layers # an array of length number of layers. each value is the number \n",
    "        self.activation_function = activation_functions # a list of Tuples (a, x) where a is the activation type ('relu', 'sigmoid', 'tanh', 'lrelu') and x is the alpha range(0, 1) for lrelu \n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.initialize_weights()\n",
    "        # how should the data be inputed...\n",
    "        # here you can split the data into training and testing data\n",
    "        # I am going to make my dataset below and that will make it easier to find out what the shape is\n",
    "        \n",
    "        # STILL NEED TO CREATE MINIBATCHES\n",
    "        ## building the model\n",
    "    def initialize_weights(self): \n",
    "        last_layer = self.input_data.shape[1]\n",
    "        for i, size in enumerate(self.hidden_layers):\n",
    "            self.weights.append(0.1 * np.random.randn(last_layer, size))\n",
    "            self.biases.append(np.zeros((1, size)))\n",
    "            last_layer = self.hidden_layers[i]\n",
    "            \n",
    "        self.weights.append(0.1 * np.random.randn(last_layer, self.output_data.shape[1]))\n",
    "        self.biases.append(np.zeros((1, self.output_data.shape[1])))\n",
    "        \n",
    "                \n",
    "        # build the 1st layer with inputs\n",
    "    def activation(self, input, type):#Relu, sigmoid, tanh\n",
    "        \n",
    "        if type[0] == \"relu\":\n",
    "            return np.maximum(0, input)\n",
    "        elif type[0] == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-input))\n",
    "        elif type[0] == \"tanh\":\n",
    "            return np.tanh(input)\n",
    "        elif type[0] == \"lrelu\": # THink about how you can have an alpha parameter when you want it \n",
    "            return np.maximum(type[1] * input, input) \n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "\n",
    "    def activation_derivative(self, input, type): # check this you should remember what function you had\n",
    "        \n",
    "        if type[0] == 'relu':\n",
    "            return np.where(input > 0, 1, 0)\n",
    "        elif type[0] == 'sigmoid':\n",
    "            sig = self.activation(input, ('sigmoid', 0))\n",
    "            return sig * (1 - sig)\n",
    "        elif type[0] == 'tanh':\n",
    "            return 1 - np.tanh(input)**2\n",
    "        elif type[0] == 'lrelu':\n",
    "            dx = np.ones_like(input)\n",
    "            dx[input <= 0] = type[1]   # THink about how you can have an alpha parameter when you want it \n",
    "            return dx\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function derivative\")\n",
    "                              \n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        activations = []\n",
    "        for i, size in enumerate(self.hidden_layers):\n",
    "            z = np.dot(inputs, self.weights[i]) + self.biases[i] # dot product of the matrices \"input\" (the previous layer activations) and \"self.weights\" + self.biases which has same number of rows\n",
    "            print(f'forwardsshape{z.shape}')\n",
    "            a = self.activation(z, self.activation_function[i]) # meausiring activation\n",
    "            activations.append((z, a)) # added to an array that can be accessed later\n",
    "            inputs = a # output if the layer is the inout of the next\n",
    "            \n",
    "        z = np.dot(inputs, self.weights[-1]) + self.biases[-1]\n",
    "        print(z.shape)\n",
    "        a = z\n",
    "        activations.append((z, a))\n",
    "        print(f'activations {activations[-1][0].shape}')\n",
    "        return activations ### weights and biases are recorded in self.weights and self.biases, outputs recorded in activations\n",
    "        \n",
    "        \n",
    "    def compute_loss(self, predictions, targets):\n",
    "        if self.loss_function == 'mse':\n",
    "            return np.mean((predictions - targets) ** 2) # the loss function averaged out for normalisation\n",
    "        else:\n",
    "            raise Exception(\"Unsupported loss function\")\n",
    "\n",
    "\n",
    "    def train(self): # choosing optimizer (the method we use to train the model with)\n",
    "        if self.optimizer == \"SGD\":\n",
    "            return self.sgd_train()\n",
    "        elif self.optimizer == \"AdamW\":\n",
    "            return self.adamw_train()\n",
    "\n",
    "    def backwards(self, activations, dvalues): # activations is just the return of the forward pass, dvalue\n",
    "        # activations is an array with a tuple of (z, a) for each node in self.hidden_layers\n",
    "        # we want to go backwards through this array and calculate the gradients of z and a\n",
    "        # then create an array of grdient, weight, bias\n",
    "    \n",
    "        grads = []\n",
    "        z, a = activations[-1]  # Retrieve the output layer activations\n",
    "        delta = dvalues * self.activation_derivative(z, self.activation_function[-1])  # Compute delta for the output layer\n",
    "        print(f'delta {delta.shape}')\n",
    "        grads.append((delta, self.weights[-1], self.biases[-1]))  # Store gradients for output layer weights and biases\n",
    "\n",
    "        # Backpropagate through hidden layers\n",
    "        for i in reversed(range(len(self.hidden_layers))):\n",
    "            z, a_prev = activations[i]\n",
    "            delta = np.dot(delta, self.weights[i + 1].T) * self.activation_derivative(z, self.activation_function[i])\n",
    "            print(f'layerdelta {delta.shape}')\n",
    "            grads.append((delta, self.weights[i], self.biases[i]))\n",
    "\n",
    "        return grads[::-1]  # Return gradients in reversed order for consistent weight updates\n",
    "\n",
    "\n",
    "    def sgd_train(self):\n",
    "        epoch_data = []\n",
    "        loss_history = []\n",
    "        # for i in epoch:\n",
    "        # forward pass = self.forward(self.create_minibatches(self.input_data))\n",
    "        # backwards = self.backwards(compute_loss(forward_pass))\n",
    "        # self.update_weights(backwards)\n",
    "        for epoch in range(self.epochs):\n",
    "            for start in range(0, self.input_data.shape[0], self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_inputs = self.input_data[start:end]\n",
    "                batch_outputs = self.output_data[start:end]\n",
    "    \n",
    "                \n",
    "                activations = self.forward(batch_inputs)\n",
    "                predictions = activations[-1][0]\n",
    "                print(predictions.shape, batch_outputs.shape)\n",
    "                loss = self.compute_loss(predictions, batch_outputs)\n",
    "                dvalues = 2 * (predictions - batch_outputs) / self.batch_size\n",
    "\n",
    "                grads = self.backwards(activations, dvalues)\n",
    "                self.update_weights(grads)\n",
    "\n",
    "            if epoch % 2 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "                epoch_data.append(epoch)\n",
    "                loss_history.append(loss)\n",
    "\n",
    "        plt.plot(epoch_data, loss_history)\n",
    "        plt.xlabel(\"Iteration Number\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Iteration Number vs Loss\")\n",
    "        plt.show()\n",
    "        \n",
    "    def adamw_train(self):\n",
    "        pass\n",
    "    \n",
    "    def update_weights(self, grads):\n",
    "        # print(len(self.weights))\n",
    "        # print(len(grads))\n",
    "        if self.optimizer == \"SGD\":\n",
    "            for i, (grad, weights, biases) in enumerate(grads):\n",
    "                print(self.weights[i].shape, self.learning_rate, grad.shape)\n",
    "                self.weights[i] -= self.learning_rate * grad.T\n",
    "                self.biases[i] -= self.learning_rate * np.sum(grad, axis=0, keepdims=True)\n",
    "            \n",
    "        elif self.train == \"AdamW\":\n",
    "            pass\n",
    "        #else:\n",
    "            #raise Exception(\"Invalid optimizer entered\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        activations = self.forward(inputs)\n",
    "        return activations[-1][0]\n",
    "## methods: build, train, use (give predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "fd2646f5-b04b-4298-a2ee-eee7968ed62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.uniform(-1, 1, (25, 20, 5)) #25, 20, 5\n",
    "\n",
    "Y_train = np.random.uniform(0, 1, (25, 20, 2)) # 25, 20, 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
