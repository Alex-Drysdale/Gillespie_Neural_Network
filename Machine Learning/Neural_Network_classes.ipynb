{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb65193-a360-4dca-9866-1e5d94a9790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e51ea43-a410-4c91-b12b-c917ae2d8a16",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 41 (2673206303.py, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 44\u001b[1;36m\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'if' statement on line 41\n"
     ]
    }
   ],
   "source": [
    "## lets try and make neural network class\n",
    "# input_data, output_data, [number of neurons and size], activation_function, epochs, learning_rate,  \n",
    "# optomizers: ADAMW\n",
    "class Neural_Network:\n",
    "    def __init__(self, input_data, output_data, hidden_layers, activation_function, epochs, learning_rate, optimizers, loss_function, batch_size):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        self.hidden_layers = hidden_layers # an array of length number of layers. each value is the number of nodes of that layer\n",
    "        self.activation_function = activation_function\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizers = optimizers\n",
    "        self.loss_function = loss_function\n",
    "        self.bathc_size = batch_size\n",
    "        # how should the data be inputed...\n",
    "        # here you can split the data into training and testing data\n",
    "        # I am going to make my dataset below and that will make it easier to find out what the shape is\n",
    "        \n",
    "        ## building the model\n",
    "        last_layer = self.input_data.shape[1]\n",
    "        layer_dict = {} # using a dictionary to record weights and bias values\n",
    "        for i, size in enumerate(self.hidden_layers):\n",
    "            layer_dict[f'w{i + 1}'] = 0.1 * np.random.randn(last_layer, size)\n",
    "            layer_dict[f'b{i + 1}'] = np.zeros((1, size))\n",
    "            last_layer = self.hidden_layers[i]\n",
    "            \n",
    "        layer_dict[\"weights_output\"] = 0.1 * np.random.randn(last_layer, self.output_data.shape[1]) # wacking in output data here means we have to clean before\n",
    "        layer_dict[\"bias_output\"] = np.zeros((1, self.output_data.shape))\n",
    "        \n",
    "                \n",
    "        # build the 1st layer with inputs\n",
    "    def activation(self, input, type = \"relu\", alpha = 0.2): #Relu, sigmoid, tanh\n",
    "        if type == \"relu\":\n",
    "            return np.maximum(0, input)\n",
    "        elif type == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-inputs))\n",
    "        elif type == \"tanh\":\n",
    "            return np.tanh(input)\n",
    "        elif type == \"lrelu\":\n",
    "            return np.maximum(alpha * input, input) \n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "\n",
    "    def activation_derivative(self, input, type = 'relu'): # check this you should remember what function you had\n",
    "        if type == 'relu':\n",
    "            return np.where(input > 0, 1, 0)\n",
    "        elif type == 'sigmoid':\n",
    "            sig = self.activation(input, 'sigmoid')\n",
    "            return sig * (1 - sig)\n",
    "        elif type == 'tanh':\n",
    "            return 1 - np.tanh(input)**2\n",
    "        elif type == 'lrelu':\n",
    "            dx = np.ones_like(x)\n",
    "            dx[x <= 0] = alpha\n",
    "            return dx\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "                              \n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        pass # for number of epochs forward pass, calculate loss, bacwards pass\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        pass # take parameters from the train method and then do one forwards pass to get the prediction.\n",
    "\n",
    "    def loss(self, output):\n",
    "    def train()\n",
    "## methods: build, train, use (give predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9430eae-8a7b-4da7-950b-f0648f186c96",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tadaaaa",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTadaaaa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Tadaaaa"
     ]
    }
   ],
   "source": [
    "gillespie = Neural_Network()\n",
    "\n",
    "\n",
    "gillespie.train()\n",
    "gillespie.use()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12cf39-83bd-4db3-ac42-9ff798b3b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):3\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0328cefb-22fa-4d49-8dcf-e58085c38630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initial states\n",
    "initial_states = [0, 3, 0, 0, 0, 0]\n",
    "\n",
    "# Parameters (example values, adjust as necessary)\n",
    "transcription_rate = 50\n",
    "basal_transcript_rate = 0.01\n",
    "mRNA_degradation_rate = 1\n",
    "translation_rate = 5\n",
    "protein_degradation_rate = 0.1\n",
    "n = 2  # Hill coefficient\n",
    "\n",
    "# Gillespie function\n",
    "def gillespie(initial_states, transcription_rate, basal_transcript_rate, mRNA_degradation_rate, translation_rate, protein_degradation_rate, n, max_time):\n",
    "    t = 0\n",
    "    state = np.array(initial_states)\n",
    "    times = [t]\n",
    "    states = [state.copy()]\n",
    "\n",
    "    while t < max_time:\n",
    "        TetR_mRNA, TetR_protein, cI_mRNA, cI_protein, LacI_mRNA, LacI_protein = state\n",
    "\n",
    "        # Propensities\n",
    "        a = np.array([\n",
    "            (transcription_rate / (1 + LacI_protein ** n)) + basal_transcript_rate,  # production of TetR_mRNA\n",
    "            mRNA_degradation_rate * TetR_mRNA,  # degradation of TetR_mRNA\n",
    "            translation_rate * TetR_mRNA,  # production of TetR protein\n",
    "            protein_degradation_rate * TetR_protein,  # degradation of TetR protein\n",
    "\n",
    "            (transcription_rate / (1 + TetR_protein ** n)) + basal_transcript_rate,  # production of cI_mRNA\n",
    "            mRNA_degradation_rate * cI_mRNA,  # degradation of cI_mRNA\n",
    "            translation_rate * cI_mRNA,  # production of cI protein\n",
    "            protein_degradation_rate * cI_protein,  # degradation of cI protein\n",
    "\n",
    "            (transcription_rate / (1 + cI_protein ** n)) + basal_transcript_rate,  # production of LacI_mRNA\n",
    "            mRNA_degradation_rate * LacI_mRNA,  # degradation of LacI_mRNA\n",
    "            translation_rate * LacI_mRNA,  # production of LacI protein\n",
    "            protein_degradation_rate * LacI_protein  # degradation of LacI protein\n",
    "        ])\n",
    "\n",
    "        a0 = np.sum(a)\n",
    "        if a0 == 0:\n",
    "            break\n",
    "\n",
    "        r1 = np.random.random()\n",
    "        tau = (1 / a0) * np.log(1 / r1)\n",
    "\n",
    "        r2 = np.random.uniform(0, 1)\n",
    "        cumulative_sum = np.cumsum(a)\n",
    "        reaction_index = np.searchsorted(cumulative_sum, r2 * a0)\n",
    "\n",
    "        # Update the system\n",
    "        state_changes = [\n",
    "            [1, 0, 0, 0, 0, 0],   # mRNA_TetR production\n",
    "            [-1, 0, 0, 0, 0, 0],  # mRNA_TetR degradation\n",
    "            [0, 1, 0, 0, 0, 0],   # protein_TetR production\n",
    "            [0, -1, 0, 0, 0, 0],  # protein_TetR degradation\n",
    "\n",
    "            [0, 0, 1, 0, 0, 0],   # mRNA_cI production\n",
    "            [0, 0, -1, 0, 0, 0],  # mRNA_cI degradation\n",
    "            [0, 0, 0, 1, 0, 0],   # protein_cI production\n",
    "            [0, 0, 0, -1, 0, 0],  # protein_cI degradation\n",
    "\n",
    "            [0, 0, 0, 0, 1, 0],   # mRNA_LacI production\n",
    "            [0, 0, 0, 0, -1, 0],  # mRNA_LacI degradation\n",
    "            [0, 0, 0, 0, 0, 1],   # protein_LacI production\n",
    "            [0, 0, 0, 0, 0, -1],  # protein_LacI degradation\n",
    "        ]\n",
    "\n",
    "        state += state_changes[reaction_index]\n",
    "        t += tau\n",
    "\n",
    "        times.append(t)\n",
    "        states.append(state.copy())\n",
    "\n",
    "    return np.array(times), np.array(states)\n",
    "\n",
    "\n",
    "# Simulation parameters\n",
    "max_time = 1000\n",
    "\n",
    "# Run the simulation\n",
    "times, states = gillespie(initial_states, transcription_rate, basal_transcript_rate, mRNA_degradation_rate, translation_rate, protein_degradation_rate, n, max_time)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
