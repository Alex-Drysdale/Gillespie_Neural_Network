{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb65193-a360-4dca-9866-1e5d94a9790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "3e51ea43-a410-4c91-b12b-c917ae2d8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets try and make neural network class\n",
    "# input_data, output_data, [number of neurons and size], activation_function, epochs, learning_rate,  \n",
    "# optomizers: ADAMW\n",
    "class Neural_Network:\n",
    "    def __init__(self, input_data, output_data, hidden_layers, activation_functions, epochs, learning_rate, optimizer, loss_function, batch_size):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        self.hidden_layers = hidden_layers # an array of length number of layers. each value is the number \n",
    "        self.activation_function = activation_functions # a list of Tuples (a, x) where a is the activation type ('relu', 'sigmoid', 'tanh', 'lrelu') and x is the alpha range(0, 1) for lrelu \n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.initialize_weights()\n",
    "        # how should the data be inputed...\n",
    "        # here you can split the data into training and testing data\n",
    "        # I am going to make my dataset below and that will make it easier to find out what the shape is\n",
    "        \n",
    "        # STILL NEED TO CREATE MINIBATCHES\n",
    "        ## building the model\n",
    "    def initialize_weights(self): \n",
    "        last_layer = self.input_data.shape[0] # The rows of input data (feature space)\n",
    "        for i, size in enumerate(self.hidden_layers):\n",
    "            self.weights.append(0.1 * np.random.randn(size, last_layer)) # (neuron number, feature space)\n",
    "            self.biases.append(np.zeros((size, 1)))\n",
    "            last_layer = self.hidden_layers[i]\n",
    "            \n",
    "        self.weights.append(0.1 * np.random.randn(self.output_data.shape[0], last_layer)) # last layer has shape (neuron number, \n",
    "        self.biases.append(np.zeros((self.output_data.shape[0], 1)))\n",
    "        print(f'last weights {self.weights[-1].shape}')\n",
    "        print(f' first weights {self.weights[0].shape}')\n",
    "                \n",
    "        # build the 1st layer with inputs\n",
    "    def activation(self, input, type):#Relu, sigmoid, tanh\n",
    "        \n",
    "        if type[0] == \"relu\":\n",
    "            return np.maximum(0, input)\n",
    "        elif type[0] == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-input))\n",
    "        elif type[0] == \"tanh\":\n",
    "            return np.tanh(input)\n",
    "        elif type[0] == \"lrelu\": # THink about how you can have an alpha parameter when you want it \n",
    "            return np.maximum(type[1] * input, input) \n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "\n",
    "    def activation_derivative(self, input, type): # check this you should remember what function you had\n",
    "        \n",
    "        if type[0] == 'relu':\n",
    "            return np.where(input > 0, 1, 0)\n",
    "        elif type[0] == 'sigmoid':\n",
    "            sig = self.activation(input, ('sigmoid', 0))\n",
    "            return sig * (1 - sig)\n",
    "        elif type[0] == 'tanh':\n",
    "            return 1 - np.tanh(input)**2\n",
    "        elif type[0] == 'lrelu':\n",
    "            dx = np.ones_like(input)\n",
    "            dx[input <= 0] = type[1]   # THink about how you can have an alpha parameter when you want it \n",
    "            return dx\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function derivative\")\n",
    "                              \n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        activations = []\n",
    "        for i, size in enumerate(self.hidden_layers):\n",
    "            z = np.dot(self.weights[i], inputs) + self.biases[i] # dot product of the matrices \"input\" (the previous layer activations) and \"self.weights\" + self.biases which has same number of rows\n",
    "            print(f'forwardsshape{z.shape}')\n",
    "            a = self.activation(z, self.activation_function[i]) # measuring activation\n",
    "            activations.append((z, a)) # added to an array that can be accessed later\n",
    "            inputs = a # output if the layer is the inout of the next\n",
    "            \n",
    "        z = np.dot(self.weights[-1], inputs) + self.biases[-1]\n",
    "        print(f'z_shape_output {z.shape}')\n",
    "        a = z\n",
    "        activations.append((z, a))\n",
    "        print(f'activations {activations[-1][0].shape}')\n",
    "        return activations ### weights and biases are recorded in self.weights and self.biases, outputs recorded in activations\n",
    "        \n",
    "        \n",
    "    def compute_loss(self, predictions, targets):\n",
    "        if self.loss_function == 'mse':\n",
    "            return np.mean((predictions - targets) ** 2) # the loss function averaged out for normalisation\n",
    "        else:\n",
    "            raise Exception(\"Unsupported loss function\")\n",
    "            \n",
    "    def loss_backwards(self, predictions, targets):\n",
    "        if self.loss_function == 'mse':\n",
    "            return 2 * (predictions - targets) / len(targets) # the loss function averaged out for normalisation\n",
    "        else:\n",
    "            raise Exception(\"Unsupported loss function\")\n",
    "\n",
    "    def train(self): # choosing optimizer (the method we use to train the model with)\n",
    "        if self.optimizer == \"SGD\":\n",
    "            return self.sgd_train()\n",
    "        elif self.optimizer == \"AdamW\":\n",
    "            return self.adamw_train()\n",
    "\n",
    "    def backwards(self, activations, dvalues):\n",
    "        grads = []\n",
    "        z, a = activations[-1]\n",
    "        delta = dvalues * self.activation_derivative(z, self.activation_function[-1])\n",
    "        print(f'delta shape: {delta.shape}, z shape: {z.shape}')\n",
    "        grads.append((delta, self.weights[-1], self.biases[-1]))\n",
    "    \n",
    "        for i in reversed(range(len(self.hidden_layers))):\n",
    "            z, a_prev = activations[i]\n",
    "            dadz = self.activation_derivative(z, self.activation_function[i])\n",
    "            print(f'Layer {i}, z shape: {z.shape}, dadz shape: {dadz.shape}')\n",
    "    \n",
    "            # Backpropagate delta\n",
    "            delta = np.dot(self.weights[i + 1].T, delta)\n",
    "            print(f'Post dot delta shape: {delta.shape}')\n",
    "            \n",
    "            # Element-wise multiply with activation derivative\n",
    "            delta *= dadz\n",
    "            print(f'Post multiply delta shape: {delta.shape}')\n",
    "    \n",
    "            grads.append((delta, self.weights[i], self.biases[i]))\n",
    "\n",
    "        \n",
    "        return grads\n",
    "\n",
    "\n",
    "    def sgd_train(self):\n",
    "        epoch_data = []\n",
    "        loss_history = []\n",
    "        # for i in epoch:\n",
    "        # forward pass = self.forward(self.create_minibatches(self.input_data))\n",
    "        # backwards = self.backwards(compute_loss(forward_pass))\n",
    "        # self.update_weights(backwards)\n",
    "        for epoch in range(self.epochs):\n",
    "            for start in range(0, self.input_data.shape[1], self.batch_size):\n",
    "                start = start\n",
    "                end = start + self.batch_size\n",
    "                print(f'end {end}')\n",
    "                batch_inputs = self.input_data[start:end]\n",
    "                print(f'batchinputs {batch_inputs.shape}')\n",
    "                batch_outputs = self.output_data[start:end]\n",
    "                print(f'batchoutputs {len(batch_outputs)}')\n",
    "    \n",
    "                \n",
    "                activations = self.forward(batch_inputs)\n",
    "                predictions = activations[-1][1]\n",
    "                print(f' predictions {predictions.shape}')\n",
    "                loss_backwards = self.loss_backwards(predictions, batch_outputs)\n",
    "                print(f'loss_backwards {loss_backwards.shape}')\n",
    "                loss = self.compute_loss(predictions, batch_outputs)\n",
    "                \n",
    "\n",
    "                grads = self.backwards(activations, loss_backwards)\n",
    "                print(f' grads first {grads[0][0].shape}')\n",
    "                self.update_weights(grads[::-1])# , start, end)\n",
    "\n",
    "            if epoch % 2 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "                epoch_data.append(epoch)\n",
    "                loss_history.append(loss)\n",
    "\n",
    "        plt.plot(epoch_data, loss_history)\n",
    "        plt.xlabel(\"Iteration Number\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Iteration Number vs Loss\")\n",
    "        plt.show()\n",
    "        \n",
    "    def adamw_train(self):\n",
    "        pass\n",
    "    \n",
    "    def update_weights(self, grads):#, start, end):\n",
    "        # print(len(self.weights))\n",
    "        # print(len(grads))\n",
    "        if self.optimizer == \"SGD\":\n",
    "            for i, (grad, weights, biases) in enumerate(grads):\n",
    "                print(self.weights[i].shape, self.learning_rate, grad.shape)\n",
    "                self.weights[i] -= self.learning_rate * grad @ self.input_data.T   #[start:end] ## don't need to transpose grad but it is currently (last_layer, input_size) and we want it to have the shape (last_layer\n",
    "                self.biases[i] -= self.learning_rate * np.sum(grad, axis=0, keepdims=True)\n",
    "            \n",
    "        elif self.train == \"AdamW\":\n",
    "            pass\n",
    "        #else:\n",
    "            #raise Exception(\"Invalid optimizer entered\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        activations = self.forward(inputs)\n",
    "        return activations[-1][0]\n",
    "## methods: build, train, use (give predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "fd2646f5-b04b-4298-a2ee-eee7968ed62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.uniform(-1, 1, (5, 500)) #25, 20, 5\n",
    "Y_train = np.random.uniform(0, 1, (2, 500)) # 25, 20, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "5b54074d-b6a3-4b70-8d19-8fce1ad3f2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last weights (2, 30)\n",
      " first weights (30, 5)\n",
      "end 20\n",
      "batchinputs (5, 500)\n",
      "batchoutputs 2\n",
      "forwardsshape(30, 500)\n",
      "z_shape_output (2, 500)\n",
      "activations (2, 500)\n",
      " predictions (2, 500)\n",
      "loss_backwards (2, 500)\n",
      "delta shape: (2, 500), z shape: (2, 500)\n",
      "Layer 0, z shape: (30, 500), dadz shape: (30, 500)\n",
      "Post dot delta shape: (30, 500)\n",
      "Post multiply delta shape: (30, 500)\n",
      " grads first (2, 500)\n",
      "(30, 5) 0.1 (30, 500)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (30,1) doesn't match the broadcast shape (30,500)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[456], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test \u001b[38;5;241m=\u001b[39m Neural_Network(X_train, Y_train, [\u001b[38;5;241m30\u001b[39m], [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)], \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m test\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[455], line 99\u001b[0m, in \u001b[0;36mNeural_Network.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;66;03m# choosing optimizer (the method we use to train the model with)\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 99\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msgd_train()\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdamW\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madamw_train()\n",
      "Cell \u001b[1;32mIn[455], line 157\u001b[0m, in \u001b[0;36mNeural_Network.sgd_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackwards(activations, loss_backwards)\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m grads first \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_weights(grads[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;66;03m# , start, end)\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[455], line 180\u001b[0m, in \u001b[0;36mNeural_Network.update_weights\u001b[1;34m(self, grads)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i]\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate, grad\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m grad \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_data\u001b[38;5;241m.\u001b[39mT   \u001b[38;5;66;03m#[start:end] ## don't need to transpose grad but it is currently (last_layer, input_size) and we want it to have the shape (last_layer\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(grad, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdamW\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (30,1) doesn't match the broadcast shape (30,500)"
     ]
    }
   ],
   "source": [
    "test = Neural_Network(X_train, Y_train, [30], [(\"relu\", 0), (\"relu\", 0)], 10, 0.1, 'SGD', \"mse\", 20)\n",
    "test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923924cb-9414-45b1-aaf1-5d14658c5fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
